"""
Advanced Feature Selection Module
Phase 2.5 - Day 6

Implements multiple feature selection methods with aggressive reduction.
Integrates with existing feature_engineering_v2.py
"""

import logging
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass
from enum import Enum
import warnings
from pathlib import Path
import json

# Use lazy imports for heavy ML libraries
from ..utils.lazy_imports import (
    np, pd, StandardScaler, RandomForestClassifier, RandomForestRegressor, 
    XGBClassifier, XGBRegressor
)

warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)


# Lazy import functions for sklearn modules
def _lazy_sklearn_feature_selection():
    from sklearn.feature_selection import (
        SelectKBest, f_classif, mutual_info_classif, mutual_info_regression,
        RFE, RFECV, VarianceThreshold, SelectFromModel
    )
    return {
        'SelectKBest': SelectKBest,
        'f_classif': f_classif,
        'mutual_info_classif': mutual_info_classif,
        'mutual_info_regression': mutual_info_regression,
        'RFE': RFE,
        'RFECV': RFECV,
        'VarianceThreshold': VarianceThreshold,
        'SelectFromModel': SelectFromModel
    }

def _lazy_sklearn_linear():
    from sklearn.linear_model import Lasso, LassoCV, ElasticNet
    return {
        'Lasso': Lasso,
        'LassoCV': LassoCV,
        'ElasticNet': ElasticNet
    }

def _lazy_sklearn_model_selection():
    from sklearn.model_selection import TimeSeriesSplit
    return {'TimeSeriesSplit': TimeSeriesSplit}


class SelectionMethod(Enum):
    """Feature selection methods"""
    MUTUAL_INFORMATION = "mutual_information"
    LASSO = "lasso"
    RFE = "rfe" 
    RANDOM_FOREST = "random_forest"
    ENSEMBLE = "ensemble"


@dataclass
class FeatureSelectorConfig:
    """Configuration for feature selector"""
    
    # Target number of features (more aggressive)
    n_features_target: int = 50  # Was 100 in v1
    n_features_min: int = 20    # Was 50 in v1 
    n_features_max: int = 100   # Was 200 in v1
    
    # Importance threshold (more aggressive than before)
    min_importance_threshold: float = 0.005  # Was 0.001 in v1
    
    # Correlation threshold (more aggressive than before)
    correlation_threshold: float = 0.7  # Was 0.95 in v1
    
    # Selection methods
    primary_method: SelectionMethod = SelectionMethod.ENSEMBLE
    ensemble_methods: List[SelectionMethod] = None
    
    # Lasso parameters
    lasso_alpha: float = 0.01
    lasso_cv_folds: int = 5
    
    # RFE parameters
    rfe_step: float = 0.1  # Remove 10% features each step
    rfe_cv: bool = True
    
    # Mutual information parameters
    mi_neighbors: int = 3
    mi_random_state: int = 42
    
    # Validation
    cv_folds: int = 5
    
    def __post_init__(self):
        if self.ensemble_methods is None:
            self.ensemble_methods = [
                SelectionMethod.MUTUAL_INFORMATION,
                SelectionMethod.LASSO,
                SelectionMethod.RFE,
                SelectionMethod.RANDOM_FOREST
            ]


class AdvancedFeatureSelector:
    """
    Advanced feature selector with multiple methods and aggressive reduction.
    """
    
    def __init__(self, config: Optional[FeatureSelectorConfig] = None):
        self.config = config or FeatureSelectorConfig()
        
        # Caching
        self._sklearn_fs = None
        self._sklearn_linear = None
        self._sklearn_ms = None
        
    @property
    def sklearn_fs(self):
        if self._sklearn_fs is None:
            self._sklearn_fs = _lazy_sklearn_feature_selection()
        return self._sklearn_fs
        
    @property 
    def sklearn_linear(self):
        if self._sklearn_linear is None:
            self._sklearn_linear = _lazy_sklearn_linear()
        return self._sklearn_linear
        
    @property
    def sklearn_ms(self):
        if self._sklearn_ms is None:
            self._sklearn_ms = _lazy_sklearn_model_selection()
        return self._sklearn_ms
        
    def select_features(self, X: pd.DataFrame, y: pd.Series) -> Tuple[List[str], Dict[str, Any]]:
        """
        Select features using configured method(s).
        
        Args:
            X: Feature dataframe
            y: Target series
            
        Returns:
            Tuple of (selected_features, selection_metadata)
        """
        logger.info(f"Starting feature selection on {len(X.columns)} features")
        
        # Remove constant and low variance features first
        X_cleaned = self._remove_low_variance(X)
        logger.debug(f"After variance filter: {len(X_cleaned.columns)} features")
        
        # Remove highly correlated features
        X_cleaned = self._remove_correlated_features(X_cleaned)
        logger.debug(f"After correlation filter: {len(X_cleaned.columns)} features")
        
        # Apply main selection method
        if self.config.primary_method == SelectionMethod.ENSEMBLE:
            features, scores = self._select_by_ensemble(X_cleaned, y)
        elif self.config.primary_method == SelectionMethod.MUTUAL_INFORMATION:
            features, scores = self._select_by_mutual_information(X_cleaned, y)
        elif self.config.primary_method == SelectionMethod.LASSO:
            features, scores = self._select_by_lasso(X_cleaned, y)
        elif self.config.primary_method == SelectionMethod.RFE:
            features, scores = self._select_by_rfe(X_cleaned, y)
        elif self.config.primary_method == SelectionMethod.RANDOM_FOREST:
            features, scores = self._select_by_random_forest(X_cleaned, y)
        else:
            raise ValueError(f"Unknown selection method: {self.config.primary_method}")
            
        # Validation
        self._validate_selection(features, len(X.columns))
        
        metadata = self._create_metadata(features, scores, len(X.columns))
        
        logger.info(f"Feature selection complete: {len(features)} features selected")
        
        return features, metadata
    
    def _remove_low_variance(self, X: pd.DataFrame) -> pd.DataFrame:
        """Remove features with low variance"""
        selector = self.sklearn_fs['VarianceThreshold'](threshold=self.config.min_importance_threshold)
        X_transformed = selector.fit_transform(X.fillna(0))
        
        selected_features = X.columns[selector.get_support()].tolist()
        
        logger.debug(f"Removed {len(X.columns) - len(selected_features)} low variance features")
        
        return X[selected_features]
    
    def _remove_correlated_features(self, X: pd.DataFrame) -> pd.DataFrame:
        """Remove highly correlated features"""
        corr_matrix = X.corr().abs()
        
        # Find pairs with correlation above threshold
        upper_tri = corr_matrix.where(
            np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)
        )
        
        to_drop = [column for column in upper_tri.columns 
                  if any(upper_tri[column] > self.config.correlation_threshold)]
        
        if to_drop:
            logger.debug(f"Removing {len(to_drop)} highly correlated features")
        
        return X.drop(columns=list(to_drop))
    
    def _select_by_mutual_information(self, X: pd.DataFrame, y: pd.Series) -> Tuple[List[str], pd.DataFrame]:
        """Select features using mutual information"""
        # Determine if classification or regression
        is_classification = len(np.unique(y)) <= 10
        
        if is_classification:
            mi_scores = self.sklearn_fs['mutual_info_classif'](
                X.fillna(0), y,
                n_neighbors=self.config.mi_neighbors,
                random_state=self.config.mi_random_state
            )
        else:
            mi_scores = self.sklearn_fs['mutual_info_regression'](
                X.fillna(0), y,
                n_neighbors=self.config.mi_neighbors,
                random_state=self.config.mi_random_state
            )
        
        # Create scores DataFrame
        scores_df = pd.DataFrame({
            'feature': X.columns,
            'mi_score': mi_scores
        }).sort_values('mi_score', ascending=False)
        
        # Select top features
        n_features = min(self.config.n_features_target, len(scores_df))
        selected = scores_df.head(n_features)['feature'].tolist()
        
        return selected, scores_df
    
    def _select_by_lasso(self, X: pd.DataFrame, y: pd.Series) -> Tuple[List[str], pd.DataFrame]:
        """Select features using Lasso regularization"""
        # Scale features for Lasso
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X.fillna(0))
        
        # Use LassoCV to find optimal alpha
        lasso_cv = self.sklearn_linear['LassoCV'](
            cv=self.config.lasso_cv_folds,
            random_state=42,
            max_iter=1000
        )
        lasso_cv.fit(X_scaled, y)
        
        # Get coefficients
        coefficients = np.abs(lasso_cv.coef_)
        
        # Create scores DataFrame
        scores_df = pd.DataFrame({
            'feature': X.columns,
            'lasso_coef': coefficients
        }).sort_values('lasso_coef', ascending=False)
        
        # Select non-zero coefficients
        selected = scores_df[scores_df['lasso_coef'] > 0]['feature'].tolist()
        
        # If too many features, take top N
        if len(selected) > self.config.n_features_max:
            selected = scores_df.head(self.config.n_features_target)['feature'].tolist()
        # If too few, add more
        elif len(selected) < self.config.n_features_min:
            selected = scores_df.head(self.config.n_features_min)['feature'].tolist()
        
        logger.info(f"Lasso selected {len(selected)} features with alpha={lasso_cv.alpha_:.4f}")
        
        return selected, scores_df
    
    def _select_by_rfe(self, X: pd.DataFrame, y: pd.Series) -> Tuple[List[str], pd.DataFrame]:
        """Select features using Recursive Feature Elimination"""
        # Use XGBoost as base estimator
        estimator = XGBRegressor(
            n_estimators=100,
            random_state=42,
            n_jobs=-1,
            verbosity=0
        )
        
        if self.config.rfe_cv:
            # Use cross-validation to find optimal number
            selector = self.sklearn_fs['RFECV'](
                estimator=estimator,
                step=self.config.rfe_step,
                cv=self.sklearn_ms['TimeSeriesSplit'](n_splits=self.config.cv_folds),
                scoring='neg_mean_squared_error',
                n_jobs=-1
            )
        else:
            # Use fixed number of features
            selector = self.sklearn_fs['RFE'](
                estimator=estimator,
                n_features_to_select=self.config.n_features_target,
                step=self.config.rfe_step
            )
        
        selector.fit(X.fillna(0), y)
        
        # Get rankings
        scores_df = pd.DataFrame({
            'feature': X.columns,
            'rfe_ranking': selector.ranking_,
            'selected': selector.support_
        }).sort_values('rfe_ranking')
        
        # Get selected features
        selected = X.columns[selector.support_].tolist()
        
        logger.info(f"RFE selected {len(selected)} features")
        
        return selected, scores_df
    
    def _select_by_random_forest(self, X: pd.DataFrame, y: pd.Series) -> Tuple[List[str], pd.DataFrame]:
        """Select features using Random Forest importance"""
        # Determine task type
        is_classification = len(np.unique(y)) <= 10
        
        if is_classification:
            rf = RandomForestClassifier(
                n_estimators=100,
                random_state=42,
                n_jobs=-1
            )
        else:
            rf = RandomForestRegressor(
                n_estimators=100,
                random_state=42,
                n_jobs=-1
            )
        
        rf.fit(X.fillna(0), y)
        
        # Get feature importances
        scores_df = pd.DataFrame({
            'feature': X.columns,
            'rf_importance': rf.feature_importances_
        }).sort_values('rf_importance', ascending=False)
        
        # Select top features above threshold
        threshold = max(
            self.config.min_importance_threshold,
            scores_df['rf_importance'].quantile(0.5)  # At least median importance
        )
        
        selected = scores_df[
            scores_df['rf_importance'] >= threshold
        ]['feature'].tolist()
        
        # Enforce limits
        if len(selected) > self.config.n_features_max:
            selected = selected[:self.config.n_features_target]
        elif len(selected) < self.config.n_features_min:
            selected = scores_df.head(self.config.n_features_min)['feature'].tolist()
        
        logger.info(f"Random Forest selected {len(selected)} features (threshold: {threshold:.4f})")
        
        return selected, scores_df
    
    def _select_by_ensemble(self, X: pd.DataFrame, y: pd.Series) -> Tuple[List[str], pd.DataFrame]:
        """Select features using ensemble of methods"""
        all_selections = {}
        all_scores = {}
        
        logger.info(f"Running ensemble selection with methods: {self.config.ensemble_methods}")
        
        # Run each method
        for method in self.config.ensemble_methods:
            logger.debug(f"Running {method.value} selection...")
            
            if method == SelectionMethod.MUTUAL_INFORMATION:
                selected, scores = self._select_by_mutual_information(X, y)
                all_selections['mi'] = selected
                all_scores['mi'] = scores.set_index('feature')['mi_score']
                
            elif method == SelectionMethod.LASSO:
                selected, scores = self._select_by_lasso(X, y)
                all_selections['lasso'] = selected
                all_scores['lasso'] = scores.set_index('feature')['lasso_coef']
                
            elif method == SelectionMethod.RFE:
                selected, scores = self._select_by_rfe(X, y)
                all_selections['rfe'] = selected
                # For RFE, use inverse ranking as score (lower rank = higher score)
                rfe_scores = 1.0 / (scores.set_index('feature')['rfe_ranking'] + 1)
                all_scores['rfe'] = rfe_scores
                
            elif method == SelectionMethod.RANDOM_FOREST:
                selected, scores = self._select_by_random_forest(X, y)
                all_selections['rf'] = selected
                all_scores['rf'] = scores.set_index('feature')['rf_importance']
        
        # Combine selections using voting
        feature_votes = {}
        for method_name, features in all_selections.items():
            for feature in features:
                feature_votes[feature] = feature_votes.get(feature, 0) + 1
        
        # Sort by votes, then by average normalized scores
        ensemble_scores = []
        for feature in X.columns:
            votes = feature_votes.get(feature, 0)
            
            # Calculate average normalized score across methods
            scores_for_feature = []
            for method_name, scores_series in all_scores.items():
                if feature in scores_series.index:
                    # Normalize score to [0, 1]
                    max_score = scores_series.max()
                    min_score = scores_series.min()
                    if max_score > min_score:
                        norm_score = (scores_series[feature] - min_score) / (max_score - min_score)
                    else:
                        norm_score = 1.0 if scores_series[feature] > 0 else 0.0
                    scores_for_feature.append(norm_score)
            
            avg_score = np.mean(scores_for_feature) if scores_for_feature else 0.0
            
            ensemble_scores.append({
                'feature': feature,
                'votes': votes,
                'avg_norm_score': avg_score,
                'composite_score': votes + avg_score  # Combine voting and scoring
            })
        
        # Create final scores DataFrame
        scores_df = pd.DataFrame(ensemble_scores).sort_values(
            ['votes', 'avg_norm_score'], ascending=[False, False]
        )
        
        # Select features
        # Prefer features with multiple votes, then high scores
        min_votes = max(1, len(self.config.ensemble_methods) // 2)  # At least half methods
        
        high_vote_features = scores_df[scores_df['votes'] >= min_votes]['feature'].tolist()
        
        if len(high_vote_features) >= self.config.n_features_min:
            # We have enough high-vote features
            selected = high_vote_features[:self.config.n_features_target]
        else:
            # Not enough high-vote features, add best scoring ones
            selected = scores_df.head(self.config.n_features_target)['feature'].tolist()
        
        logger.info(f"Ensemble selected {len(selected)} features "
                   f"({len(high_vote_features)} with >= {min_votes} votes)")
        
        return selected, scores_df
    
    def _validate_selection(self, features: List[str], original_count: int) -> None:
        """Validate feature selection results"""
        if not features:
            raise ValueError("No features were selected")
            
        if len(features) < self.config.n_features_min:
            logger.warning(f"Selected {len(features)} features, below minimum {self.config.n_features_min}")
            
        if len(features) > self.config.n_features_max:
            logger.warning(f"Selected {len(features)} features, above maximum {self.config.n_features_max}")
            
        reduction_ratio = len(features) / original_count
        logger.info(f"Feature reduction: {original_count} -> {len(features)} ({reduction_ratio:.1%} retained)")
    
    def _create_metadata(self, features: List[str], scores: pd.DataFrame, original_count: int) -> Dict[str, Any]:
        """Create selection metadata"""
        return {
            'selected_features': features,
            'n_features_selected': len(features),
            'n_features_original': original_count,
            'reduction_ratio': len(features) / original_count,
            'selection_method': self.config.primary_method.value,
            'config': {
                'n_features_target': self.config.n_features_target,
                'min_importance_threshold': self.config.min_importance_threshold,
                'correlation_threshold': self.config.correlation_threshold
            },
            'feature_scores': scores.to_dict('records') if scores is not None else None
        }


# Example usage and testing
if __name__ == "__main__":
    # Test with synthetic data
    from sklearn.datasets import make_regression
    
    # Generate test data
    X_test, y_test = make_regression(
        n_samples=1000, 
        n_features=200,
        n_informative=20,
        noise=0.1,
        random_state=42
    )
    
    X_test = pd.DataFrame(X_test, columns=[f'feature_{i}' for i in range(200)])
    y_test = pd.Series(y_test)
    
    # Test feature selection
    config = FeatureSelectorConfig(
        n_features_target=30,
        primary_method=SelectionMethod.ENSEMBLE
    )
    
    selector = AdvancedFeatureSelector(config)
    selected_features, metadata = selector.select_features(X_test, y_test)
    
    print(f"Selected {len(selected_features)} features:")
    print(selected_features[:10])
    print(f"Reduction ratio: {metadata['reduction_ratio']:.1%}")
