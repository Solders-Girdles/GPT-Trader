"""
Main feature engineering pipeline for ML models
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
import logging
from datetime import datetime
import json

from .technical import TechnicalFeatureEngineer
from .market_regime import MarketRegimeFeatures
from ..base import FeatureEngineer
from ...core.base import ComponentConfig


class FeatureEngineeringPipeline(FeatureEngineer):
    """Main pipeline for generating and managing ML features"""

    def __init__(self, config: Optional[ComponentConfig] = None, db_manager=None):
        """Initialize feature engineering pipeline

        Args:
            config: Optional component configuration
            db_manager: Database manager for feature storage
        """
        if config is None:
            config = ComponentConfig(
                component_id='feature_pipeline',
                component_type='feature_engineer'
            )
        super().__init__(config)

        self.db_manager = db_manager
        self.logger = logging.getLogger(__name__)

        # Initialize sub-engineers
        self.technical_engineer = TechnicalFeatureEngineer()
        self.regime_engineer = MarketRegimeFeatures()

        # Feature configuration
        self.feature_config = {
            'technical': True,
            'regime': True,
            'interactions': True,
            'lags': [1, 2, 5],
            'rolling_windows': [5, 10, 20]
        }

        # Feature metadata
        self.feature_metadata = {}
        self.feature_importance = {}

    def generate_features(self, data: pd.DataFrame,
                        symbol: Optional[str] = None,
                        store_features: bool = True) -> pd.DataFrame:
        """Generate all features from raw data

        Args:
            data: DataFrame with OHLCV data
            symbol: Optional symbol identifier
            store_features: Whether to store features in database

        Returns:
            DataFrame with all engineered features
        """
        self.logger.info(f"Generating features for {len(data)} data points")

        # Initialize features DataFrame
        features = pd.DataFrame(index=data.index)

        # Generate base features
        if self.feature_config['technical']:
            technical_features = self.technical_engineer.generate_features(data)
            features = pd.concat([features, technical_features], axis=1)
            self.logger.debug(f"Generated {len(technical_features.columns)} technical features")

        if self.feature_config['regime']:
            regime_features = self.regime_engineer.generate_features(data)
            features = pd.concat([features, regime_features], axis=1)
            self.logger.debug(f"Generated {len(regime_features.columns)} regime features")

        # Generate interaction features
        if self.feature_config['interactions']:
            interaction_features = self._generate_interaction_features(features)
            features = pd.concat([features, interaction_features], axis=1)
            self.logger.debug(f"Generated {len(interaction_features.columns)} interaction features")

        # Generate lag features
        if self.feature_config['lags']:
            lag_features = self._generate_lag_features(features)
            features = pd.concat([features, lag_features], axis=1)
            self.logger.debug(f"Generated {len(lag_features.columns)} lag features")

        # Generate rolling features
        if self.feature_config['rolling_windows']:
            rolling_features = self._generate_rolling_features(features)
            features = pd.concat([features, rolling_features], axis=1)
            self.logger.debug(f"Generated {len(rolling_features.columns)} rolling features")

        # Clean features
        features = self._clean_features(features)

        # Update metadata
        self._update_metadata(features)

        # Store features if requested
        if store_features and symbol and self.db_manager:
            self._store_features(symbol, features)

        # Cache features
        self.feature_cache[data.index[-1] if len(data) > 0 else datetime.now()] = features

        self.logger.info(f"Generated total of {len(features.columns)} features")

        return features

    def _generate_interaction_features(self, features: pd.DataFrame) -> pd.DataFrame:
        """Generate interaction features between different feature groups

        Args:
            features: DataFrame with base features

        Returns:
            DataFrame with interaction features
        """
        interactions = pd.DataFrame(index=features.index)

        # Define key interactions
        interaction_pairs = [
            ('rsi_14', 'volatility_20d'),
            ('macd', 'volume_ratio'),
            ('adx', 'returns_20d'),
            ('bb_percent_20', 'volume_ratio_20d'),
            ('momentum_composite', 'high_vol_regime'),
            ('trend_consistency', 'efficiency_ratio')
        ]

        for feat1, feat2 in interaction_pairs:
            if feat1 in features.columns and feat2 in features.columns:
                # Multiplicative interaction
                interactions[f'{feat1}_x_{feat2}'] = features[feat1] * features[feat2]

                # Ratio interaction
                denominator = features[feat2].replace(0, np.nan)
                interactions[f'{feat1}_div_{feat2}'] = features[feat1] / denominator

        # Cross-sectional features
        if 'volatility_20d' in features.columns and 'returns_20d' in features.columns:
            interactions['sharpe_20d'] = features['returns_20d'] / (features['volatility_20d'] + 1e-10)

        if 'high_vol_regime' in features.columns and 'ma_trend_20_50' in features.columns:
            interactions['regime_trend_interaction'] = (
                features['high_vol_regime'] * features['ma_trend_20_50']
            )

        return interactions

    def _generate_lag_features(self, features: pd.DataFrame) -> pd.DataFrame:
        """Generate lagged features

        Args:
            features: DataFrame with base features

        Returns:
            DataFrame with lagged features
        """
        lag_features = pd.DataFrame(index=features.index)

        # Select important features to lag
        important_features = [
            'rsi_14', 'macd', 'volatility_20d', 'returns_5d',
            'volume_ratio', 'ma_trend_20_50', 'momentum_composite'
        ]

        available_features = [f for f in important_features if f in features.columns]

        for feature in available_features:
            for lag in self.feature_config['lags']:
                lag_features[f'{feature}_lag{lag}'] = features[feature].shift(lag)

                # Difference from lagged value
                lag_features[f'{feature}_diff_lag{lag}'] = (
                    features[feature] - features[feature].shift(lag)
                )

        return lag_features

    def _generate_rolling_features(self, features: pd.DataFrame) -> pd.DataFrame:
        """Generate rolling window features

        Args:
            features: DataFrame with base features

        Returns:
            DataFrame with rolling features
        """
        rolling_features = pd.DataFrame(index=features.index)

        # Select features for rolling calculations
        rolling_candidates = [
            'returns_1d', 'volatility_20d', 'volume_ratio',
            'rsi_14', 'momentum_composite'
        ]

        available_features = [f for f in rolling_candidates if f in features.columns]

        for feature in available_features:
            for window in self.feature_config['rolling_windows']:
                # Rolling statistics
                rolling_features[f'{feature}_roll_mean_{window}'] = (
                    features[feature].rolling(window).mean()
                )
                rolling_features[f'{feature}_roll_std_{window}'] = (
                    features[feature].rolling(window).std()
                )
                rolling_features[f'{feature}_roll_min_{window}'] = (
                    features[feature].rolling(window).min()
                )
                rolling_features[f'{feature}_roll_max_{window}'] = (
                    features[feature].rolling(window).max()
                )

                # Z-score
                mean = rolling_features[f'{feature}_roll_mean_{window}']
                std = rolling_features[f'{feature}_roll_std_{window}']
                rolling_features[f'{feature}_zscore_{window}'] = (
                    (features[feature] - mean) / (std + 1e-10)
                )

        return rolling_features

    def _clean_features(self, features: pd.DataFrame) -> pd.DataFrame:
        """Clean and preprocess features

        Args:
            features: DataFrame with raw features

        Returns:
            Cleaned DataFrame
        """
        # Remove infinite values
        features = features.replace([np.inf, -np.inf], np.nan)

        # Handle NaN values
        # Forward fill for most features
        features = features.ffill()

        # Fill remaining NaN with 0 (typically at the beginning)
        features = features.fillna(0)

        # Remove constant features
        constant_features = features.columns[features.std() == 0]
        if len(constant_features) > 0:
            self.logger.debug(f"Removing {len(constant_features)} constant features")
            features = features.drop(columns=constant_features)

        # Remove highly correlated features (correlation > 0.95)
        corr_matrix = features.corr().abs()
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]

        if to_drop:
            self.logger.debug(f"Removing {len(to_drop)} highly correlated features")
            features = features.drop(columns=to_drop)

        return features

    def _store_features(self, symbol: str, features: pd.DataFrame):
        """Store features in database

        Args:
            symbol: Symbol identifier
            features: DataFrame with features to store
        """
        if not self.db_manager:
            return

        records = []
        batch_size = 1000

        for timestamp, row in features.iterrows():
            for feature_name, value in row.items():
                if pd.notna(value) and np.isfinite(value):
                    records.append({
                        'symbol': symbol,
                        'feature_date': timestamp,
                        'feature_name': feature_name,
                        'feature_value': float(value),
                        'feature_group': self._get_feature_group(feature_name)
                    })

            # Batch insert
            if len(records) >= batch_size:
                self._batch_insert_features(records)
                records = []

        # Insert remaining records
        if records:
            self._batch_insert_features(records)

        self.logger.info(f"Stored {len(features.columns) * len(features)} features for {symbol}")

    def _batch_insert_features(self, records: List[Dict]):
        """Batch insert features into database

        Args:
            records: List of feature records
        """
        try:
            # Use INSERT OR REPLACE to handle duplicates
            for record in records:
                self.db_manager.execute(
                    """INSERT OR REPLACE INTO feature_store
                       (symbol, feature_date, feature_name, feature_value, feature_group)
                       VALUES (?, ?, ?, ?, ?)""",
                    (record['symbol'], record['feature_date'], record['feature_name'],
                     record['feature_value'], record.get('feature_group', 'misc'))
                )
        except Exception as e:
            self.logger.error(f"Error storing features: {e}")

    def _get_feature_group(self, feature_name: str) -> str:
        """Determine feature group from feature name

        Args:
            feature_name: Name of the feature

        Returns:
            Feature group name
        """
        if any(ind in feature_name for ind in ['rsi', 'macd', 'stoch', 'williams']):
            return 'momentum'
        elif any(ind in feature_name for ind in ['sma', 'ema', 'ma_', 'adx', 'trend']):
            return 'trend'
        elif any(ind in feature_name for ind in ['volatility', 'atr', 'bb_', 'vol_']):
            return 'volatility'
        elif any(ind in feature_name for ind in ['volume', 'obv', 'ad', 'mfi']):
            return 'volume'
        elif any(ind in feature_name for ind in ['returns', 'momentum', 'roc']):
            return 'returns'
        elif any(ind in feature_name for ind in ['regime', 'entropy', 'hurst']):
            return 'regime'
        elif '_x_' in feature_name or '_div_' in feature_name:
            return 'interaction'
        elif 'lag' in feature_name:
            return 'lag'
        elif 'roll_' in feature_name or 'zscore' in feature_name:
            return 'rolling'
        else:
            return 'misc'

    def _update_metadata(self, features: pd.DataFrame):
        """Update feature metadata

        Args:
            features: DataFrame with features
        """
        self.feature_metadata = {
            'n_features': len(features.columns),
            'feature_names': list(features.columns),
            'feature_groups': {},
            'generation_time': datetime.now().isoformat(),
            'data_points': len(features)
        }

        # Count features by group
        for feature in features.columns:
            group = self._get_feature_group(feature)
            self.feature_metadata['feature_groups'][group] = (
                self.feature_metadata['feature_groups'].get(group, 0) + 1
            )

    def get_feature_metadata(self) -> Dict[str, Any]:
        """Get feature metadata

        Returns:
            Dictionary with feature metadata
        """
        return self.feature_metadata

    def load_features(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """Load features from database

        Args:
            symbol: Symbol identifier
            start_date: Start date for features
            end_date: End date for features

        Returns:
            DataFrame with features
        """
        if not self.db_manager:
            raise ValueError("Database manager not configured")

        query = """
            SELECT feature_date, feature_name, feature_value
            FROM feature_store
            WHERE symbol = ?
            AND feature_date >= ?
            AND feature_date <= ?
            ORDER BY feature_date, feature_name
        """

        results = self.db_manager.fetch_all(query, (symbol, start_date, end_date))

        if not results:
            return pd.DataFrame()

        # Convert to DataFrame
        df = pd.DataFrame(results)

        # Pivot to wide format
        features = df.pivot(
            index='feature_date',
            columns='feature_name',
            values='feature_value'
        )

        # Convert index to datetime
        features.index = pd.to_datetime(features.index)

        return features

    def select_features(self, features: pd.DataFrame,
                       method: str = 'importance',
                       n_features: int = 50) -> List[str]:
        """Select most important features

        Args:
            features: DataFrame with all features
            method: Selection method ('importance', 'correlation', 'variance')
            n_features: Number of features to select

        Returns:
            List of selected feature names
        """
        if method == 'variance':
            # Select features with highest variance
            variances = features.var()
            selected = variances.nlargest(n_features).index.tolist()

        elif method == 'correlation':
            # Select features with low correlation to each other
            corr_matrix = features.corr().abs()
            selected = []
            remaining = list(features.columns)

            while len(selected) < n_features and remaining:
                # Add feature with lowest average correlation to selected
                if not selected:
                    # Start with feature with highest variance
                    next_feat = features[remaining].var().idxmax()
                else:
                    # Find feature with lowest correlation to selected
                    avg_corr = corr_matrix.loc[remaining, selected].mean(axis=1)
                    next_feat = avg_corr.idxmin()

                selected.append(next_feat)
                remaining.remove(next_feat)

        else:  # importance
            # Use stored importance or calculate based on variance
            if self.feature_importance:
                importance_df = pd.Series(self.feature_importance)
                selected = importance_df.nlargest(n_features).index.tolist()
            else:
                # Fallback to variance
                return self.select_features(features, method='variance', n_features=n_features)

        return selected
