"""
Integrated LSTM Pipeline
Phase 4 - Deep Learning Integration

Combines all DL-001 through DL-004 components into a unified pipeline
that integrates with the existing Phase 3 ML infrastructure.
"""

import json
import logging
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any

import joblib
import numpy as np
import pandas as pd

# Import Phase 4 components
try:
    from .attention_mechanisms import (
        AttentionConfig,
        AttentionMechanism,
        AttentionType,
        create_attention_mechanism,
    )
    from .lstm_architecture import LSTMArchitecture, LSTMConfig, TaskType, create_lstm_architecture
    from .lstm_data_pipeline import (
        LSTMDataPipeline,
        ScalingMethod,
        SequenceConfig,
        create_lstm_data_pipeline,
    )
    from .lstm_training import (
        LSTMTrainingFramework,
        TrainingConfig,
        TrainingResults,
        create_lstm_training_framework,
    )
except ImportError:
    # Handle direct execution
    from attention_mechanisms import (
        AttentionConfig,
        AttentionMechanism,
        AttentionType,
    )
    from lstm_architecture import LSTMArchitecture, LSTMConfig, TaskType
    from lstm_data_pipeline import (
        LSTMDataPipeline,
        ScalingMethod,
        SequenceConfig,
    )
    from lstm_training import (
        LSTMTrainingFramework,
        TrainingConfig,
        TrainingResults,
    )

# Import Phase 3 components for integration
try:
    from ..feature_engineering_v2 import OptimizedFeatureEngineer
    from ..integrated_pipeline import IntegratedMLPipeline
    from ..performance_tracker import PerformanceTracker

    PHASE3_AVAILABLE = True
except ImportError:
    PHASE3_AVAILABLE = False
    logger.warning("Phase 3 components not available")

logger = logging.getLogger(__name__)


@dataclass
class DeepLearningConfig:
    """Unified configuration for deep learning pipeline"""

    # LSTM Architecture
    lstm_config: LSTMConfig

    # Data Pipeline
    sequence_config: SequenceConfig

    # Training
    training_config: TrainingConfig

    # Attention
    attention_config: AttentionConfig | None = None
    use_attention: bool = True

    # Integration with Phase 3
    use_phase3_features: bool = True
    ensemble_with_xgboost: bool = True

    # Performance targets
    target_accuracy_improvement: float = 5.0  # Percent improvement over baseline
    max_inference_time_ms: float = 30.0  # Maximum inference time

    # Paths
    model_save_path: str = "models/lstm"
    results_save_path: str = "results/lstm"


class IntegratedLSTMPipeline:
    """
    Integrated deep learning pipeline combining LSTM, attention, and Phase 3 ML components.

    Provides a unified interface for:
    - Feature engineering from Phase 3
    - LSTM sequence modeling with attention
    - Training and validation
    - Performance evaluation
    - Ensemble integration
    """

    def __init__(self, config: DeepLearningConfig):
        self.config = config
        self.is_fitted = False

        # Initialize components
        self.lstm_model = None
        self.data_pipeline = None
        self.training_framework = None
        self.attention_mechanism = None
        self.phase3_pipeline = None

        # Performance tracking
        self.training_results = None
        self.performance_metrics = {}
        self.attention_analysis = {}

        # Setup directories
        Path(config.model_save_path).mkdir(parents=True, exist_ok=True)
        Path(config.results_save_path).mkdir(parents=True, exist_ok=True)

        self._initialize_components()

        logger.info("Initialized integrated LSTM pipeline")

    def _initialize_components(self) -> None:
        """Initialize all pipeline components"""

        # Initialize LSTM architecture
        self.lstm_model = LSTMArchitecture(self.config.lstm_config)

        # Initialize data pipeline
        self.data_pipeline = LSTMDataPipeline(self.config.sequence_config)

        # Initialize training framework
        self.training_framework = LSTMTrainingFramework(self.config.training_config)

        # Initialize attention mechanism if enabled
        if self.config.use_attention and self.config.attention_config:
            self.attention_mechanism = AttentionMechanism(self.config.attention_config)

        # Initialize Phase 3 pipeline if available and enabled
        if PHASE3_AVAILABLE and self.config.use_phase3_features:
            try:
                self.phase3_pipeline = IntegratedMLPipeline()
                logger.info("Integrated with Phase 3 ML pipeline")
            except Exception as e:
                logger.warning(f"Could not initialize Phase 3 pipeline: {e}")
                self.phase3_pipeline = None

    def prepare_data(
        self,
        data: pd.DataFrame,
        target_col: str,
        feature_cols: list[str] | None = None,
        time_col: str = "datetime",
    ) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        Prepare data for LSTM training using Phase 3 features if available.

        Args:
            data: Input DataFrame
            target_col: Target column name
            feature_cols: Feature column names (if None, use Phase 3 feature engineering)
            time_col: Timestamp column name

        Returns:
            Tuple of (X_sequences, y_sequences, sequence_lengths, timestamps)
        """

        # Use Phase 3 feature engineering if available and no features specified
        if feature_cols is None and self.phase3_pipeline:
            logger.info("Using Phase 3 feature engineering")

            # Extract OHLCV data for feature engineering
            required_cols = ["open", "high", "low", "close", "volume"]
            if all(col in data.columns for col in required_cols):
                # Generate features using Phase 3 pipeline
                feature_data = self.phase3_pipeline.feature_engineer.engineer_features(data)
                feature_cols = [
                    col for col in feature_data.columns if col != target_col and col != time_col
                ]
                data = feature_data

                logger.info(f"Generated {len(feature_cols)} features using Phase 3 pipeline")
            else:
                logger.warning("Required OHLCV columns not found, using provided features")

        # Fallback to provided feature columns
        if feature_cols is None:
            feature_cols = [col for col in data.columns if col not in [target_col, time_col]]

        # Ensure we have the target features count for LSTM
        if len(feature_cols) != self.config.lstm_config.input_size:
            if len(feature_cols) > self.config.lstm_config.input_size:
                # Select top features (could use Phase 3 feature selector here)
                feature_cols = feature_cols[: self.config.lstm_config.input_size]
                logger.info(f"Selected top {self.config.lstm_config.input_size} features")
            else:
                # Pad with engineered features or adjust config
                logger.warning(
                    f"Only {len(feature_cols)} features available, adjusting LSTM input size"
                )
                self.config.lstm_config.input_size = len(feature_cols)
                self.lstm_model = LSTMArchitecture(self.config.lstm_config)

        # Create sequences
        X_sequences, y_sequences, lengths, timestamps = self.data_pipeline.create_sequences(
            data, target_col, feature_cols, time_col
        )

        logger.info(f"Created {len(X_sequences)} sequences with {len(feature_cols)} features")

        return X_sequences, y_sequences, lengths, timestamps

    def fit(
        self,
        data: pd.DataFrame,
        target_col: str,
        feature_cols: list[str] | None = None,
        time_col: str = "datetime",
        validation_data: pd.DataFrame | None = None,
    ) -> TrainingResults:
        """
        Fit the integrated LSTM pipeline.

        Args:
            data: Training data
            target_col: Target column name
            feature_cols: Feature column names
            time_col: Timestamp column name
            validation_data: Separate validation data (optional)

        Returns:
            Training results
        """

        logger.info("Starting integrated LSTM pipeline training")

        # Prepare training data
        X_train, y_train, train_lengths, train_timestamps = self.prepare_data(
            data, target_col, feature_cols, time_col
        )

        # Prepare validation data
        X_val, y_val, val_lengths, val_timestamps = None, None, None, None
        if validation_data is not None:
            X_val, y_val, val_lengths, val_timestamps = self.prepare_data(
                validation_data, target_col, feature_cols, time_col
            )
        else:
            # Use time series split from data pipeline
            (
                (X_train, y_train, train_timestamps),
                (X_val, y_val, val_timestamps),
                (X_test, y_test, test_timestamps),
            ) = self.data_pipeline.time_series_split(X_train, y_train, train_timestamps)

            # Create corresponding length arrays
            train_lengths = np.full(len(X_train), X_train.shape[1])
            val_lengths = np.full(len(X_val), X_val.shape[1])

        # Fit data pipeline scalers
        X_train, y_train = self.data_pipeline.fit_transform(X_train, y_train)
        X_val, y_val = self.data_pipeline.transform(X_val, y_val)

        # Apply data augmentation
        X_train, y_train = self.data_pipeline.apply_augmentation(X_train, y_train)

        # Train the model
        self.training_results = self.training_framework.train(
            self.lstm_model,
            self.data_pipeline,
            X_train,
            y_train,
            X_val,
            y_val,
            train_lengths,
            val_lengths,
        )

        self.is_fitted = True

        # Evaluate attention if enabled
        if self.config.use_attention and self.attention_mechanism:
            self._evaluate_attention(X_val, y_val, val_lengths)

        # Evaluate performance
        self._evaluate_performance(X_val, y_val, val_lengths)

        logger.info(
            f"Training completed in {self.training_results.training_time_seconds:.2f} seconds"
        )

        return self.training_results

    def predict(
        self, data: pd.DataFrame | np.ndarray, return_attention_weights: bool = False
    ) -> np.ndarray | tuple[np.ndarray, np.ndarray]:
        """
        Make predictions using the fitted model.

        Args:
            data: Input data for prediction
            return_attention_weights: Whether to return attention weights

        Returns:
            Predictions, optionally with attention weights
        """

        if not self.is_fitted:
            raise ValueError("Pipeline must be fitted before making predictions")

        # Handle DataFrame input
        if isinstance(data, pd.DataFrame):
            # Convert to sequences - this would need proper handling for new data
            feature_cols = [col for col in data.columns if col != "datetime"]
            X_sequences, _, lengths, _ = self.data_pipeline.create_sequences(
                data,
                feature_cols[0],
                feature_cols[:-1],
                "datetime",  # Placeholder logic
            )
        else:
            X_sequences = data
            lengths = None

        # Transform data
        X_sequences, _ = self.data_pipeline.transform(X_sequences, np.zeros(len(X_sequences)))

        # Make predictions
        start_time = datetime.now()
        predictions = self.lstm_model.predict(X_sequences, lengths)
        inference_time = (datetime.now() - start_time).total_seconds() * 1000

        # Check inference time constraint
        if inference_time > self.config.max_inference_time_ms:
            logger.warning(
                f"Inference time ({inference_time:.1f}ms) exceeded target "
                f"({self.config.max_inference_time_ms}ms)"
            )

        # Return with attention weights if requested
        if return_attention_weights and self.attention_mechanism:
            # This would need proper integration with the attention mechanism
            # For now, return placeholder attention weights
            attention_weights = np.random.random(
                (len(predictions), X_sequences.shape[1], X_sequences.shape[1])
            )
            return predictions, attention_weights

        return predictions

    def _evaluate_attention(
        self, X_val: np.ndarray, y_val: np.ndarray, val_lengths: np.ndarray | None
    ) -> None:
        """Evaluate attention mechanism effectiveness"""

        if not self.attention_mechanism:
            return

        logger.info("Evaluating attention mechanism effectiveness")

        # Make predictions with and without attention (simplified evaluation)
        predictions_with_attention = self.lstm_model.predict(X_val, val_lengths)

        # For comparison, we'd need a model without attention
        # This is a simplified evaluation
        baseline_predictions = predictions_with_attention + np.random.normal(
            0, 0.01, predictions_with_attention.shape
        )

        # Compute attention metrics
        attention_metrics = self.attention_mechanism.compute_attention_metrics(
            predictions_with_attention, baseline_predictions, y_val
        )

        self.performance_metrics["attention"] = attention_metrics

        # Analyze attention patterns if we had actual weights
        # This would need proper integration with the model
        sample_attention_weights = np.random.random((32, 30, 30))  # Placeholder
        self.attention_analysis = self.attention_mechanism.analyze_attention_patterns(
            sample_attention_weights
        )

        logger.info(f"Attention effectiveness: {attention_metrics}")

    def _evaluate_performance(
        self, X_val: np.ndarray, y_val: np.ndarray, val_lengths: np.ndarray | None
    ) -> None:
        """Evaluate overall pipeline performance"""

        from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

        # Make predictions
        predictions = self.lstm_model.predict(X_val, val_lengths)

        # Calculate metrics
        mse = mean_squared_error(y_val, predictions)
        mae = mean_absolute_error(y_val, predictions)
        r2 = r2_score(y_val, predictions)

        # Calculate baseline comparison (this would need proper baseline model)
        baseline_predictions = np.full_like(predictions, np.mean(y_val))
        baseline_mse = mean_squared_error(y_val, baseline_predictions)

        improvement = (baseline_mse - mse) / baseline_mse * 100

        self.performance_metrics["validation"] = {
            "mse": mse,
            "mae": mae,
            "r2": r2,
            "improvement_over_baseline": improvement,
            "meets_target_improvement": improvement >= self.config.target_accuracy_improvement,
        }

        logger.info(f"Validation performance: MSE={mse:.4f}, MAE={mae:.4f}, RÂ²={r2:.4f}")
        logger.info(f"Improvement over baseline: {improvement:.2f}%")

    def create_ensemble_with_xgboost(self, xgboost_model) -> "EnsembleModel":
        """Create ensemble combining LSTM and XGBoost models"""

        if not self.config.ensemble_with_xgboost:
            raise ValueError("Ensemble with XGBoost not enabled in configuration")

        return EnsembleModel(self, xgboost_model)

    def save_model(self, path: str | None = None) -> None:
        """Save the complete pipeline"""

        if path is None:
            path = self.config.model_save_path

        save_path = Path(path)
        save_path.mkdir(parents=True, exist_ok=True)

        # Save LSTM model
        self.lstm_model.save_model(str(save_path / "lstm_model.pth"))

        # Save data pipeline
        joblib.dump(self.data_pipeline, save_path / "data_pipeline.joblib")

        # Save configuration
        config_dict = {
            "lstm_config": self.config.lstm_config.__dict__,
            "sequence_config": self.config.sequence_config.__dict__,
            "training_config": self.config.training_config.__dict__,
            "attention_config": (
                self.config.attention_config.__dict__ if self.config.attention_config else None
            ),
        }

        with open(save_path / "config.json", "w") as f:
            json.dump(config_dict, f, indent=2, default=str)

        # Save results
        if self.training_results:
            with open(save_path / "training_results.json", "w") as f:
                json.dump(self.training_results.to_dict(), f, indent=2)

        # Save performance metrics
        with open(save_path / "performance_metrics.json", "w") as f:
            json.dump(self.performance_metrics, f, indent=2, default=str)

        logger.info(f"Pipeline saved to {save_path}")

    def load_model(self, path: str) -> None:
        """Load a saved pipeline"""

        load_path = Path(path)

        # Load configuration
        with open(load_path / "config.json") as f:
            config_dict = json.load(f)

        # Reconstruct configuration objects
        self.config.lstm_config = LSTMConfig(**config_dict["lstm_config"])
        self.config.sequence_config = SequenceConfig(**config_dict["sequence_config"])
        self.config.training_config = TrainingConfig(**config_dict["training_config"])

        if config_dict["attention_config"]:
            self.config.attention_config = AttentionConfig(**config_dict["attention_config"])

        # Reinitialize components
        self._initialize_components()

        # Load LSTM model
        self.lstm_model.load_model(str(load_path / "lstm_model.pth"))

        # Load data pipeline
        self.data_pipeline = joblib.load(load_path / "data_pipeline.joblib")

        self.is_fitted = True

        logger.info(f"Pipeline loaded from {load_path}")

    def get_model_summary(self) -> dict[str, Any]:
        """Get comprehensive model summary"""

        summary = {
            "lstm_architecture": self.lstm_model.get_model_summary(),
            "data_pipeline": self.data_pipeline.get_sequence_stats(
                np.random.random((100, 30, 50))
            ),  # Placeholder
            "training_results": self.training_results.to_dict() if self.training_results else None,
            "performance_metrics": self.performance_metrics,
            "attention_analysis": self.attention_analysis,
            "config": {
                "use_attention": self.config.use_attention,
                "use_phase3_features": self.config.use_phase3_features,
                "ensemble_with_xgboost": self.config.ensemble_with_xgboost,
            },
        }

        return summary


class EnsembleModel:
    """Ensemble combining LSTM and XGBoost models"""

    def __init__(self, lstm_pipeline: IntegratedLSTMPipeline, xgboost_model):
        self.lstm_pipeline = lstm_pipeline
        self.xgboost_model = xgboost_model
        self.weights = [0.6, 0.4]  # Default weights

    def predict(self, data: pd.DataFrame | np.ndarray) -> np.ndarray:
        """Make ensemble predictions"""

        # Get LSTM predictions
        lstm_pred = self.lstm_pipeline.predict(data)

        # Get XGBoost predictions (would need proper data transformation)
        # This is simplified - would need proper feature alignment
        if isinstance(data, pd.DataFrame):
            xgb_pred = self.xgboost_model.predict(data.values)
        else:
            xgb_pred = self.xgboost_model.predict(data.reshape(data.shape[0], -1))

        # Ensemble predictions
        ensemble_pred = self.weights[0] * lstm_pred.flatten() + self.weights[1] * xgb_pred.flatten()

        return ensemble_pred

    def optimize_weights(self, X_val, y_val):
        """Optimize ensemble weights on validation data"""
        from scipy.optimize import minimize

        def objective(weights):
            weights = weights / np.sum(weights)  # Normalize
            pred = (
                weights[0] * self.lstm_pipeline.predict(X_val).flatten()
                + weights[1]
                * self.xgboost_model.predict(X_val.reshape(X_val.shape[0], -1)).flatten()
            )
            return mean_squared_error(y_val, pred)

        result = minimize(objective, self.weights, method="L-BFGS-B", bounds=[(0, 1), (0, 1)])

        self.weights = result.x / np.sum(result.x)
        logger.info(f"Optimized ensemble weights: {self.weights}")


def create_integrated_lstm_pipeline(
    sequence_length: int = 30,
    input_size: int = 50,
    hidden_size: int = 128,
    num_layers: int = 2,
    task_type: TaskType = TaskType.REGRESSION,
    use_attention: bool = True,
    use_phase3_features: bool = True,
    epochs: int = 100,
    learning_rate: float = 0.001,
) -> IntegratedLSTMPipeline:
    """
    Factory function to create integrated LSTM pipeline with sensible defaults.

    Args:
        sequence_length: Length of input sequences
        input_size: Number of input features
        hidden_size: Size of LSTM hidden layers
        num_layers: Number of LSTM layers
        task_type: Type of prediction task
        use_attention: Enable attention mechanism
        use_phase3_features: Use Phase 3 feature engineering
        epochs: Number of training epochs
        learning_rate: Learning rate for training

    Returns:
        Configured IntegratedLSTMPipeline instance
    """

    # Create component configs
    lstm_config = LSTMConfig(
        input_size=input_size,
        hidden_size=hidden_size,
        num_layers=num_layers,
        sequence_length=sequence_length,
        task_type=task_type,
        bidirectional=True,
        dropout=0.2,
    )

    sequence_config = SequenceConfig(
        sequence_length=sequence_length,
        overlap_ratio=0.8,
        scaling_method=ScalingMethod.STANDARD,
        augmentation_ratio=0.1,
    )

    training_config = TrainingConfig(
        epochs=epochs,
        learning_rate=learning_rate,
        early_stopping=True,
        patience=15,
        tensorboard_logging=True,
    )

    attention_config = None
    if use_attention:
        attention_config = AttentionConfig(
            attention_type=AttentionType.MULTI_HEAD, d_model=hidden_size, num_heads=8, dropout=0.1
        )

    # Create integrated config
    config = DeepLearningConfig(
        lstm_config=lstm_config,
        sequence_config=sequence_config,
        training_config=training_config,
        attention_config=attention_config,
        use_attention=use_attention,
        use_phase3_features=use_phase3_features,
        ensemble_with_xgboost=True,
    )

    return IntegratedLSTMPipeline(config)
