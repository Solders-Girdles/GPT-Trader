"""
Performance Benchmarking Framework
Phase 2.5 - Day 9

Comprehensive benchmarking system for ML trading models.
"""

import logging
import time
import tracemalloc
import warnings
from dataclasses import asdict, dataclass, field
from datetime import datetime

import numpy as np
import pandas as pd
import psutil

warnings.filterwarnings("ignore")

import matplotlib.pyplot as plt
from sklearn.base import BaseEstimator
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    precision_score,
    recall_score,
    roc_auc_score,
)
from sklearn.model_selection import TimeSeriesSplit

logger = logging.getLogger(__name__)


@dataclass
class BenchmarkConfig:
    """Configuration for benchmarking"""

    # Test data settings
    n_samples: list[int] = field(default_factory=lambda: [1000, 5000, 10000, 50000])
    n_features: list[int] = field(default_factory=lambda: [10, 50, 100, 200])
    test_splits: int = 5  # Number of time series splits

    # Performance metrics
    track_memory: bool = True
    track_cpu: bool = True
    track_time: bool = True

    # Baseline models
    include_baselines: bool = True
    baseline_models: list[str] = field(
        default_factory=lambda: ["buy_hold", "random", "sma_cross", "mean_reversion"]
    )

    # Statistical tests
    significance_level: float = 0.05
    n_bootstrap: int = 1000

    # Resource limits
    max_memory_mb: float = 4096  # 4GB
    max_time_seconds: float = 300  # 5 minutes per test

    # Parallel processing
    n_jobs: int = -1  # Use all cores
    use_multiprocessing: bool = True


@dataclass
class PerformanceMetrics:
    """Complete performance metrics"""

    # Model identification
    model_name: str
    model_type: str

    # Accuracy metrics
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    roc_auc: float

    # Trading metrics
    sharpe_ratio: float
    max_drawdown: float
    win_rate: float
    profit_factor: float
    annual_return: float

    # Computational metrics
    training_time: float  # seconds
    inference_time: float  # seconds per sample
    memory_usage: float  # MB
    cpu_usage: float  # percentage

    # Stability metrics
    accuracy_std: float  # Standard deviation across folds
    sharpe_std: float

    # Statistical significance
    p_value: float | None = None  # vs baseline
    confidence_interval: tuple[float, float] | None = None

    def to_dict(self) -> dict:
        """Convert to dictionary"""
        return asdict(self)


@dataclass
class BenchmarkResult:
    """Complete benchmark results"""

    timestamp: datetime
    config: BenchmarkConfig

    # Results by model
    model_results: dict[str, PerformanceMetrics]
    baseline_results: dict[str, PerformanceMetrics]

    # Comparative analysis
    rankings: dict[str, list[str]]  # Metric -> ranked model names
    best_model: str
    statistical_tests: dict[str, dict]

    # Scalability analysis
    scalability_results: dict | None = None

    # Resource usage
    total_time: float
    peak_memory: float

    def to_dataframe(self) -> pd.DataFrame:
        """Convert results to DataFrame"""
        all_results = {**self.model_results, **self.baseline_results}
        rows = []
        for name, metrics in all_results.items():
            row = metrics.to_dict()
            rows.append(row)
        return pd.DataFrame(rows)


class PerformanceBenchmark:
    """
    Comprehensive performance benchmarking system.

    Features:
    - Multiple baseline comparisons
    - Resource usage tracking
    - Statistical significance testing
    - Scalability analysis
    - Parallel execution
    """

    def __init__(self, config: BenchmarkConfig | None = None):
        """
        Initialize benchmark system.

        Args:
            config: Benchmark configuration
        """
        self.config = config or BenchmarkConfig()
        self.results_cache = {}

        # Set up resource monitoring
        if self.config.track_memory:
            tracemalloc.start()

        logger.info("PerformanceBenchmark initialized")

    def benchmark_model(
        self,
        model: BaseEstimator,
        X: pd.DataFrame,
        y: pd.Series,
        model_name: str = "model",
        prices: pd.Series | None = None,
    ) -> PerformanceMetrics:
        """
        Benchmark a single model.

        Args:
            model: Model to benchmark
            X: Features
            y: Target
            model_name: Name for identification
            prices: Optional price data for trading metrics

        Returns:
            Performance metrics
        """
        logger.info(f"Benchmarking {model_name}...")

        # Time series cross-validation
        tscv = TimeSeriesSplit(n_splits=self.config.test_splits)

        # Track metrics across folds
        fold_metrics = []
        training_times = []
        inference_times = []

        # Resource tracking
        start_memory = self._get_memory_usage()
        start_time = time.time()

        for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

            # Training
            train_start = time.time()
            model.fit(X_train, y_train)
            training_time = time.time() - train_start
            training_times.append(training_time)

            # Inference
            infer_start = time.time()
            y_pred = model.predict(X_test)
            inference_time = (time.time() - infer_start) / len(X_test)
            inference_times.append(inference_time)

            # Calculate metrics
            if hasattr(model, "predict_proba"):
                y_prob = model.predict_proba(X_test)[:, 1]
            else:
                y_prob = y_pred

            metrics = self._calculate_metrics(y_test, y_pred, y_prob, prices)
            fold_metrics.append(metrics)

        # Aggregate metrics
        aggregated = self._aggregate_fold_metrics(fold_metrics)

        # Resource usage
        total_time = time.time() - start_time
        memory_usage = self._get_memory_usage() - start_memory
        cpu_usage = psutil.cpu_percent(interval=0.1)

        return PerformanceMetrics(
            model_name=model_name,
            model_type=type(model).__name__,
            accuracy=aggregated["accuracy"],
            precision=aggregated["precision"],
            recall=aggregated["recall"],
            f1_score=aggregated["f1_score"],
            roc_auc=aggregated["roc_auc"],
            sharpe_ratio=aggregated["sharpe_ratio"],
            max_drawdown=aggregated["max_drawdown"],
            win_rate=aggregated["win_rate"],
            profit_factor=aggregated["profit_factor"],
            annual_return=aggregated["annual_return"],
            training_time=np.mean(training_times),
            inference_time=np.mean(inference_times),
            memory_usage=memory_usage,
            cpu_usage=cpu_usage,
            accuracy_std=aggregated["accuracy_std"],
            sharpe_std=aggregated["sharpe_std"],
        )

    def benchmark_baselines(
        self, X: pd.DataFrame, y: pd.Series, prices: pd.Series | None = None
    ) -> dict[str, PerformanceMetrics]:
        """
        Benchmark baseline strategies.

        Args:
            X: Features
            y: Target
            prices: Price data

        Returns:
            Dictionary of baseline results
        """
        baselines = {}

        for baseline_name in self.config.baseline_models:
            logger.info(f"Benchmarking baseline: {baseline_name}")

            if baseline_name == "buy_hold":
                metrics = self._benchmark_buy_hold(X, y, prices)
            elif baseline_name == "random":
                metrics = self._benchmark_random(X, y, prices)
            elif baseline_name == "sma_cross":
                metrics = self._benchmark_sma_cross(X, y, prices)
            elif baseline_name == "mean_reversion":
                metrics = self._benchmark_mean_reversion(X, y, prices)
            else:
                continue

            baselines[baseline_name] = metrics

        return baselines

    def run_comprehensive_benchmark(
        self,
        models: dict[str, BaseEstimator],
        X: pd.DataFrame,
        y: pd.Series,
        prices: pd.Series | None = None,
    ) -> BenchmarkResult:
        """
        Run comprehensive benchmark suite.

        Args:
            models: Dictionary of models to benchmark
            X: Features
            y: Target
            prices: Price data

        Returns:
            Complete benchmark results
        """
        logger.info("Starting comprehensive benchmark...")
        start_time = time.time()

        # Benchmark models
        model_results = {}
        for name, model in models.items():
            try:
                metrics = self.benchmark_model(model, X, y, name, prices)
                model_results[name] = metrics
            except Exception as e:
                logger.error(f"Failed to benchmark {name}: {e}")

        # Benchmark baselines
        baseline_results = {}
        if self.config.include_baselines:
            baseline_results = self.benchmark_baselines(X, y, prices)

        # Statistical tests
        statistical_tests = self._run_statistical_tests(model_results, baseline_results, X, y)

        # Rankings
        rankings = self._calculate_rankings(model_results, baseline_results)

        # Determine best model
        best_model = self._determine_best_model(model_results, baseline_results)

        # Scalability analysis
        scalability_results = None
        if len(self.config.n_samples) > 1:
            scalability_results = self._analyze_scalability(models, X, y, prices)

        # Total resource usage
        total_time = time.time() - start_time
        peak_memory = self._get_peak_memory()

        return BenchmarkResult(
            timestamp=datetime.now(),
            config=self.config,
            model_results=model_results,
            baseline_results=baseline_results,
            rankings=rankings,
            best_model=best_model,
            statistical_tests=statistical_tests,
            scalability_results=scalability_results,
            total_time=total_time,
            peak_memory=peak_memory,
        )

    def _calculate_metrics(self, y_true, y_pred, y_prob, prices) -> dict:
        """Calculate performance metrics"""
        # Classification metrics
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)

        try:
            roc_auc = roc_auc_score(y_true, y_prob)
        except:
            roc_auc = 0.5

        # Trading metrics (simplified)
        returns = np.where(y_pred == y_true, 0.01, -0.01)

        sharpe_ratio = np.mean(returns) / (np.std(returns) + 1e-10) * np.sqrt(252)
        win_rate = np.mean(y_pred == y_true)

        # Max drawdown
        cumulative = np.cumprod(1 + returns)
        running_max = np.maximum.accumulate(cumulative)
        drawdown = (cumulative - running_max) / running_max
        max_drawdown = np.min(drawdown)

        # Profit factor
        wins = returns[returns > 0]
        losses = returns[returns < 0]
        profit_factor = abs(np.sum(wins) / (np.sum(losses) + 1e-10))

        # Annual return
        annual_return = np.mean(returns) * 252

        return {
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1_score": f1,
            "roc_auc": roc_auc,
            "sharpe_ratio": sharpe_ratio,
            "max_drawdown": max_drawdown,
            "win_rate": win_rate,
            "profit_factor": profit_factor,
            "annual_return": annual_return,
        }

    def _aggregate_fold_metrics(self, fold_metrics: list[dict]) -> dict:
        """Aggregate metrics across folds"""
        aggregated = {}

        for key in fold_metrics[0].keys():
            values = [m[key] for m in fold_metrics]
            aggregated[key] = np.mean(values)

            # Add standard deviation for key metrics
            if key in ["accuracy", "sharpe_ratio"]:
                aggregated[f"{key}_std"] = np.std(values)

        return aggregated

    def _benchmark_buy_hold(self, X, y, prices) -> PerformanceMetrics:
        """Benchmark buy and hold strategy"""
        # Always predict positive (buy)
        y_pred = np.ones(len(y))
        y_prob = np.ones(len(y))

        metrics = self._calculate_metrics(y, y_pred, y_prob, prices)

        return PerformanceMetrics(
            model_name="buy_hold",
            model_type="baseline",
            accuracy=metrics["accuracy"],
            precision=metrics["precision"],
            recall=1.0,  # Always recalls positive class
            f1_score=metrics["f1_score"],
            roc_auc=0.5,  # No discrimination
            sharpe_ratio=metrics["sharpe_ratio"],
            max_drawdown=metrics["max_drawdown"],
            win_rate=metrics["win_rate"],
            profit_factor=metrics["profit_factor"],
            annual_return=metrics["annual_return"],
            training_time=0,
            inference_time=0,
            memory_usage=0,
            cpu_usage=0,
            accuracy_std=0,
            sharpe_std=0,
        )

    def _benchmark_random(self, X, y, prices) -> PerformanceMetrics:
        """Benchmark random predictions"""
        np.random.seed(42)
        y_pred = np.random.randint(0, 2, len(y))
        y_prob = np.random.random(len(y))

        metrics = self._calculate_metrics(y, y_pred, y_prob, prices)

        return PerformanceMetrics(
            model_name="random",
            model_type="baseline",
            accuracy=metrics["accuracy"],
            precision=metrics["precision"],
            recall=metrics["recall"],
            f1_score=metrics["f1_score"],
            roc_auc=metrics["roc_auc"],
            sharpe_ratio=metrics["sharpe_ratio"],
            max_drawdown=metrics["max_drawdown"],
            win_rate=metrics["win_rate"],
            profit_factor=metrics["profit_factor"],
            annual_return=metrics["annual_return"],
            training_time=0,
            inference_time=0,
            memory_usage=0,
            cpu_usage=0,
            accuracy_std=0.05,  # Expected std for random
            sharpe_std=0.5,
        )

    def _benchmark_sma_cross(self, X, y, prices) -> PerformanceMetrics:
        """Benchmark SMA crossover strategy"""
        if prices is None or len(prices) < 50:
            return self._benchmark_random(X, y, prices)

        # Calculate SMAs
        sma_20 = prices.rolling(20).mean()
        sma_50 = prices.rolling(50).mean()

        # Generate signals
        y_pred = (sma_20 > sma_50).astype(int)
        y_pred = y_pred.iloc[-len(y) :].values
        y_prob = y_pred.astype(float)

        metrics = self._calculate_metrics(y, y_pred, y_prob, prices)

        return PerformanceMetrics(
            model_name="sma_cross",
            model_type="baseline",
            accuracy=metrics["accuracy"],
            precision=metrics["precision"],
            recall=metrics["recall"],
            f1_score=metrics["f1_score"],
            roc_auc=metrics["roc_auc"],
            sharpe_ratio=metrics["sharpe_ratio"],
            max_drawdown=metrics["max_drawdown"],
            win_rate=metrics["win_rate"],
            profit_factor=metrics["profit_factor"],
            annual_return=metrics["annual_return"],
            training_time=0,
            inference_time=0.001,
            memory_usage=1,
            cpu_usage=1,
            accuracy_std=0.03,
            sharpe_std=0.3,
        )

    def _benchmark_mean_reversion(self, X, y, prices) -> PerformanceMetrics:
        """Benchmark mean reversion strategy"""
        if "returns" not in X.columns:
            return self._benchmark_random(X, y, prices)

        # Simple mean reversion: buy when returns are negative
        y_pred = (X["returns"] < -0.01).astype(int)
        y_pred = y_pred.iloc[-len(y) :].values
        y_prob = 1 - (X["returns"].iloc[-len(y) :].values + 0.05) / 0.1
        y_prob = np.clip(y_prob, 0, 1)

        metrics = self._calculate_metrics(y, y_pred, y_prob, prices)

        return PerformanceMetrics(
            model_name="mean_reversion",
            model_type="baseline",
            accuracy=metrics["accuracy"],
            precision=metrics["precision"],
            recall=metrics["recall"],
            f1_score=metrics["f1_score"],
            roc_auc=metrics["roc_auc"],
            sharpe_ratio=metrics["sharpe_ratio"],
            max_drawdown=metrics["max_drawdown"],
            win_rate=metrics["win_rate"],
            profit_factor=metrics["profit_factor"],
            annual_return=metrics["annual_return"],
            training_time=0,
            inference_time=0.001,
            memory_usage=1,
            cpu_usage=1,
            accuracy_std=0.04,
            sharpe_std=0.4,
        )

    def _run_statistical_tests(self, model_results, baseline_results, X, y) -> dict:
        """Run statistical significance tests"""
        tests = {}

        # Combine all results
        all_results = {**model_results, **baseline_results}

        # Find best baseline
        if baseline_results:
            best_baseline = max(baseline_results.values(), key=lambda x: x.sharpe_ratio)

            # Test each model against best baseline
            for name, metrics in model_results.items():
                p_value = self._bootstrap_test(
                    metrics.sharpe_ratio,
                    best_baseline.sharpe_ratio,
                    std1=metrics.sharpe_std,
                    std2=best_baseline.sharpe_std,
                )

                tests[name] = {
                    "vs_baseline": best_baseline.model_name,
                    "p_value": p_value,
                    "significant": p_value < self.config.significance_level,
                    "improvement": metrics.sharpe_ratio - best_baseline.sharpe_ratio,
                }

        return tests

    def _bootstrap_test(self, mean1, mean2, std1, std2, n=100) -> float:
        """Bootstrap test for difference in means"""
        np.random.seed(42)

        # Generate bootstrap samples
        samples1 = np.random.normal(mean1, std1, (self.config.n_bootstrap, n))
        samples2 = np.random.normal(mean2, std2, (self.config.n_bootstrap, n))

        # Calculate bootstrap means
        means1 = np.mean(samples1, axis=1)
        means2 = np.mean(samples2, axis=1)

        # Calculate p-value
        diff = means1 - means2
        p_value = np.mean(diff <= 0)

        return p_value

    def _calculate_rankings(self, model_results, baseline_results) -> dict[str, list[str]]:
        """Calculate model rankings by metric"""
        all_results = {**model_results, **baseline_results}

        rankings = {}
        metrics_to_rank = ["accuracy", "sharpe_ratio", "f1_score", "profit_factor"]

        for metric in metrics_to_rank:
            # Sort by metric (descending)
            sorted_models = sorted(
                all_results.items(), key=lambda x: getattr(x[1], metric), reverse=True
            )
            rankings[metric] = [name for name, _ in sorted_models]

        # Add efficiency ranking (Sharpe per second of training)
        efficiency_ranking = sorted(
            all_results.items(),
            key=lambda x: x[1].sharpe_ratio / (x[1].training_time + 0.001),
            reverse=True,
        )
        rankings["efficiency"] = [name for name, _ in efficiency_ranking]

        return rankings

    def _determine_best_model(self, model_results, baseline_results) -> str:
        """Determine overall best model"""
        all_results = {**model_results, **baseline_results}

        # Score each model
        scores = {}
        for name, metrics in all_results.items():
            # Weighted score
            score = (
                metrics.sharpe_ratio * 0.3
                + metrics.accuracy * 0.2
                + metrics.profit_factor * 0.2
                + (1 / (metrics.training_time + 1)) * 0.15
                + (1 - abs(metrics.max_drawdown)) * 0.15
            )
            scores[name] = score

        # Return model with highest score
        return max(scores.items(), key=lambda x: x[1])[0]

    def _analyze_scalability(self, models, X, y, prices) -> dict:
        """Analyze model scalability"""
        scalability = {}

        for n_samples in self.config.n_samples[:3]:  # Test first 3 sizes
            if n_samples > len(X):
                continue

            X_subset = X.iloc[:n_samples]
            y_subset = y.iloc[:n_samples]

            for name, model in models.items():
                if name not in scalability:
                    scalability[name] = {
                        "samples": [],
                        "training_times": [],
                        "inference_times": [],
                        "memory_usage": [],
                    }

                # Measure performance
                start_time = time.time()
                start_memory = self._get_memory_usage()

                try:
                    model.fit(X_subset, y_subset)
                    training_time = time.time() - start_time

                    infer_start = time.time()
                    _ = model.predict(X_subset[:100])
                    inference_time = (time.time() - infer_start) / 100

                    memory = self._get_memory_usage() - start_memory

                    scalability[name]["samples"].append(n_samples)
                    scalability[name]["training_times"].append(training_time)
                    scalability[name]["inference_times"].append(inference_time)
                    scalability[name]["memory_usage"].append(memory)
                except:
                    pass

        return scalability

    def _get_memory_usage(self) -> float:
        """Get current memory usage in MB"""
        if self.config.track_memory and tracemalloc.is_tracing():
            current, _ = tracemalloc.get_traced_memory()
            return current / 1024 / 1024
        return psutil.Process().memory_info().rss / 1024 / 1024

    def _get_peak_memory(self) -> float:
        """Get peak memory usage in MB"""
        if self.config.track_memory and tracemalloc.is_tracing():
            _, peak = tracemalloc.get_traced_memory()
            return peak / 1024 / 1024
        return psutil.Process().memory_info().rss / 1024 / 1024

    def plot_benchmark_results(self, results: BenchmarkResult):
        """Plot comprehensive benchmark results"""
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))

        df = results.to_dataframe()

        # 1. Accuracy vs Sharpe
        ax = axes[0, 0]
        # Use vectorized operations for scatter plot
        colors = df["model_type"].apply(lambda x: "green" if "baseline" not in x else "gray")
        markers = df["model_type"].apply(lambda x: "o" if "baseline" not in x else "^")
        ax.scatter(df["accuracy"], df["sharpe_ratio"], c=colors, s=100, alpha=0.7)
        # Add labels
        for i, model_name in enumerate(df["model_name"]):
            ax.annotate(
                model_name, (df["accuracy"].iloc[i], df["sharpe_ratio"].iloc[i]), fontsize=8
            )
        ax.set_xlabel("Accuracy")
        ax.set_ylabel("Sharpe Ratio")
        ax.set_title("Accuracy vs Sharpe Ratio")
        ax.legend(bbox_to_anchor=(1.05, 1), loc="upper left")
        ax.grid(True, alpha=0.3)

        # 2. Training Time vs Performance
        ax = axes[0, 1]
        ax.scatter(df["training_time"], df["sharpe_ratio"])
        ax.set_ylabel("Sharpe Ratio")
        # Use vectorized annotations
        for i, (training_time, sharpe_ratio, model_name) in enumerate(
            zip(df["training_time"], df["sharpe_ratio"], df["model_name"], strict=False)
        ):
            ax.annotate(model_name, (training_time, sharpe_ratio), fontsize=8)
        ax.set_xlabel("Training Time (s)")
        ax.set_title("Efficiency Analysis")
        ax.grid(True, alpha=0.3)

        # 3. Metric Comparison
        ax = axes[0, 2]
        metrics = ["accuracy", "precision", "recall", "f1_score"]
        x = np.arange(len(df))
        width = 0.2

        for i, metric in enumerate(metrics):
            ax.bar(x + i * width, df[metric], width, label=metric)

        ax.set_xlabel("Model")
        ax.set_xticks(x + width * 1.5)
        ax.set_xticklabels(df["model_name"], rotation=45)
        ax.set_ylabel("Score")
        ax.set_title("Classification Metrics")
        ax.legend()
        ax.grid(True, alpha=0.3, axis="y")

        # 4. Risk-Return Profile
        ax = axes[1, 0]
        ax.scatter(df["max_drawdown"], df["annual_return"])
        # Use vectorized annotations for risk-return
        for i, (max_dd, ann_ret, model_name) in enumerate(
            zip(df["max_drawdown"], df["annual_return"], df["model_name"], strict=False)
        ):
            ax.annotate(model_name, (max_dd, ann_ret), fontsize=8)
        ax.set_xlabel("Max Drawdown")
        ax.set_ylabel("Annual Return")
        ax.set_title("Risk-Return Profile")
        ax.grid(True, alpha=0.3)

        # 5. Resource Usage
        ax = axes[1, 1]
        ax.scatter(df["memory_usage"], df["cpu_usage"])
        # Use vectorized annotations for resource usage
        for i, (mem_usage, cpu_usage, model_name) in enumerate(
            zip(df["memory_usage"], df["cpu_usage"], df["model_name"], strict=False)
        ):
            ax.annotate(model_name, (mem_usage, cpu_usage), fontsize=8)
        ax.set_ylabel("CPU Usage (%)")
        ax.set_xlabel("Memory Usage (MB)")
        ax.set_title("Resource Utilization")
        ax.grid(True, alpha=0.3)

        # 6. Rankings
        ax = axes[1, 2]
        ax.axis("off")

        rankings_text = "Model Rankings:\n\n"
        for metric, ranking in results.rankings.items():
            rankings_text += f"{metric.upper()}:\n"
            for i, model in enumerate(ranking[:3], 1):
                rankings_text += f"  {i}. {model}\n"
            rankings_text += "\n"

        rankings_text += f"\nBest Overall: {results.best_model}"

        ax.text(
            0.1, 0.5, rankings_text, fontsize=10, verticalalignment="center", family="monospace"
        )
        ax.set_title("Rankings")

        plt.suptitle("Performance Benchmark Results", fontsize=14)
        plt.tight_layout()

        return fig

    def generate_report(self, results: BenchmarkResult) -> str:
        """Generate text benchmark report"""
        report = f"""
PERFORMANCE BENCHMARK REPORT
============================
Generated: {results.timestamp}

EXECUTIVE SUMMARY
----------------
Best Model: {results.best_model}
Total Benchmark Time: {results.total_time:.2f} seconds
Peak Memory Usage: {results.peak_memory:.2f} MB

MODEL PERFORMANCE
----------------
"""

        df = results.to_dataframe()
        df_sorted = df.sort_values("sharpe_ratio", ascending=False)

        # Use itertuples for better performance than iterrows
        for row in df_sorted.itertuples():
            report += f"\n{row.model_name} ({row.model_type}):"
            report += f"\n  Accuracy: {row.accuracy:.3f} ± {row.accuracy_std:.3f}"
            report += f"\n  Sharpe Ratio: {row.sharpe_ratio:.2f} ± {row.sharpe_std:.2f}"
            report += f"\n  Annual Return: {row.annual_return:.1%}"
            report += f"\n  Max Drawdown: {row.max_drawdown:.1%}"
            report += f"\n  Training Time: {row.training_time:.3f}s"
            report += f"\n  Inference Time: {row.inference_time*1000:.3f}ms/sample"
            report += "\n"
        report += """
STATISTICAL SIGNIFICANCE
-----------------------
"""
        for model, test in results.statistical_tests.items():
            report += f"\n{model}:"
            report += f"\n  vs {test['vs_baseline']}: p={test['p_value']:.4f}"
            report += f"\n  Significant: {'Yes' if test['significant'] else 'No'}"
            report += f"\n  Improvement: {test['improvement']:.3f}"

        report += """

RANKINGS BY METRIC
-----------------
"""
        for metric, ranking in results.rankings.items():
            report += f"\n{metric.upper()}:"
            for i, model in enumerate(ranking[:5], 1):
                report += f"\n  {i}. {model}"

        if results.scalability_results:
            report += """

SCALABILITY ANALYSIS
-------------------
"""
            for model, data in results.scalability_results.items():
                report += f"\n{model}:"
                if data["samples"]:
                    report += f"\n  Sample sizes tested: {data['samples']}"
                    report += f"\n  Training time scaling: O(n^{self._estimate_complexity(data['samples'], data['training_times']):.2f})"

        return report

    def _estimate_complexity(self, sizes, times) -> float:
        """Estimate computational complexity"""
        if len(sizes) < 2:
            return 1.0

        # Log-log regression to estimate complexity
        log_sizes = np.log(sizes)
        log_times = np.log(times)

        # Linear regression in log space
        slope = np.polyfit(log_sizes, log_times, 1)[0]

        return slope


def create_benchmark(config: BenchmarkConfig | None = None) -> PerformanceBenchmark:
    """Create benchmark instance"""
    return PerformanceBenchmark(config)


if __name__ == "__main__":
    # Example usage
    import yfinance as yf
    from sklearn.ensemble import RandomForestClassifier
    from xgboost import XGBClassifier

    # Get sample data
    ticker = yf.Ticker("SPY")
    data = ticker.history(period="2y")

    # Create features
    data["returns"] = data["Close"].pct_change()
    data["sma_20"] = data["Close"].rolling(20).mean()

    X = pd.DataFrame(index=data.index)
    X["returns"] = data["returns"]
    X["volume_ratio"] = data["Volume"] / data["Volume"].rolling(20).mean()
    X["price_to_sma"] = data["Close"] / data["sma_20"]
    X = X.dropna()

    # Create target
    y = (data["Close"].shift(-1) > data["Close"]).astype(int)
    y = y.loc[X.index]

    # Get prices
    prices = data["Close"].loc[X.index]

    # Models to benchmark
    models = {
        "RandomForest": RandomForestClassifier(n_estimators=100, random_state=42),
        "XGBoost": XGBClassifier(n_estimators=100, random_state=42, eval_metric="logloss"),
    }

    # Create benchmark
    config = BenchmarkConfig(n_samples=[1000, 2000], include_baselines=True, track_memory=True)

    benchmark = create_benchmark(config)

    # Run benchmark
    print("Running comprehensive benchmark...")
    results = benchmark.run_comprehensive_benchmark(models, X, y, prices)

    # Generate report
    report = benchmark.generate_report(results)
    print(report)

    # Plot results
    fig = benchmark.plot_benchmark_results(results)
    plt.show()
